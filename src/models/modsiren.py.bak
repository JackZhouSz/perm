import numpy as np
import torch
import torch.nn as nn

from models.embedding import FourierFeatMapping, IdentityMapping, PositionalEncoding


class SineLayer(nn.Module):
    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):
        super().__init__()
        self.omega_0 = omega_0
        self.is_first = is_first

        self.in_features = in_features
        self.linear = nn.Linear(in_features, out_features, bias=bias)

        self.init_weights()

    def init_weights(self):
        with torch.no_grad():
            if self.is_first:
                self.linear.weight.uniform_(-1 / self.in_features,
                                            1 / self.in_features)
            else:
                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,
                                            np.sqrt(6 / self.in_features) / self.omega_0)

    def forward(self, input):
        return torch.sin(self.omega_0 * self.linear(input))


class SineMLP(nn.Module):
    def __init__(self, in_features, out_features, hidden_layers,
                 hidden_features, outermost_linear=False, first_omega_0=30,
                 hidden_omega_0=30.):
        super().__init__()

        self.net = []
        self.net.append(SineLayer(in_features, hidden_features,
                                  is_first=True, omega_0=first_omega_0))

        for i in range(hidden_layers):
            self.net.append(SineLayer(hidden_features, hidden_features,
                                      is_first=False, omega_0=hidden_omega_0))

        if outermost_linear:
            final_linear = nn.Linear(hidden_features, out_features)

            with torch.no_grad():
                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0,
                                             np.sqrt(6 / hidden_features) / hidden_omega_0)

            self.net.append(final_linear)
        else:
            self.net.append(SineLayer(hidden_features, out_features,
                                      is_first=False, omega_0=hidden_omega_0))

        self.net = nn.Sequential(*self.net)

    def forward(self, coords, gating_layers=None):
        output = self.net(coords)
        return output


class GatedSineMLP(nn.Module):
    def __init__(self, in_features, out_features, hidden_layers, hidden_features, outermost_linear=False,
                 first_omega_0=30., hidden_omega_0=30.):
        super().__init__()

        self.hidden_layers = hidden_layers
        self.first_layer = SineLayer(in_features, hidden_features,
                                     is_first=True, omega_0=first_omega_0)

        for i in range(hidden_layers):
            layer = SineLayer(hidden_features, hidden_features,
                              is_first=False, omega_0=hidden_omega_0)
            setattr(self, f'hidden_layer_{i}', layer)

        if outermost_linear:
            final_linear = nn.Linear(hidden_features, out_features)

            with torch.no_grad():
                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0,
                                             np.sqrt(6 / hidden_features) / hidden_omega_0)

            self.final_layer = final_linear
        else:
            self.final_layer = SineLayer(hidden_features, out_features,
                                         is_first=False, omega_0=hidden_omega_0)

    def forward(self, coords, gating_layers):
        out = self.first_layer(coords)
        for i in range(self.hidden_layers):
            out = gating_layers[i] * getattr(self, f'hidden_layer_{i}')(out)
        output = self.final_layer(out)
        return output


class GatedReluMLP(nn.Module):
    def __init__(self, in_features, out_features, hidden_layers, hidden_features, outermost_linear=False,
                 first_omega_0=30., hidden_omega_0=30.):
        super().__init__()

        self.hidden_layers = hidden_layers
        self.first_layer = nn.Sequential(nn.Linear(in_features, hidden_features),
                                         nn.ReLU(True))

        for i in range(hidden_layers):
            layer = nn.Sequential(nn.Linear(hidden_features, hidden_features),
                                  nn.ReLU(True))
            setattr(self, f'hidden_layer_{i}', layer)

        self.final_layer = nn.Sequential(
            nn.Linear(hidden_features, out_features))

    def forward(self, coords, gating_layers):
        out = self.first_layer(coords)
        for i in range(self.hidden_layers):
            out = gating_layers[i] * getattr(self, f'hidden_layer_{i}')(out)
        output = self.final_layer(out)
        return output


class ReLUMLP(nn.Module):
    def __init__(self,
                 in_features,
                 out_features,
                 hidden_layers,
                 hidden_features):
        super().__init__()

        self.first_layer = nn.Sequential(nn.Linear(in_features, hidden_features),
                                         nn.ReLU())
        self.hidden_layers = hidden_layers

        for i in range(hidden_layers):
            layer = nn.Sequential(
                nn.Linear(hidden_features, hidden_features), nn.ReLU())
            setattr(self, f'hidden_layer_{i}', layer)

        self.final_layer = nn.Sequential(
            nn.Linear(hidden_features, out_features))

    def forward(self, x, gating_layers=None):
        out = self.first_layer(x)
        for i in range(self.hidden_layers):
            if gating_layers:
                out = gating_layers[i] * getattr(self, f'hidden_layer_{i}')(out)
            else:
                out = getattr(self, f'hidden_layer_{i}')(out)

        output = self.final_layer(out)
        return output


class ModulationMLP(nn.Module):
    def __init__(self, in_features, hidden_features, hidden_layers):
        super().__init__()

        self.first_layer = nn.Sequential(nn.Linear(in_features, hidden_features),
                                         nn.ReLU(True))
        self.hidden_layers = hidden_layers  # since there is no final layer
        for i in range(self.hidden_layers):
            layer = nn.Sequential(nn.Linear(hidden_features, hidden_features),
                                  nn.ReLU(True))
            setattr(self, f'layer_{i}', layer)

    def forward(self, coords):
        output = self.first_layer(coords)
        skip = output
        gating_layers = []
        for i in range(self.hidden_layers):
            output = getattr(self, f'layer_{i}')(output) + skip
            gating_layers.append(output)
        return gating_layers


class SineModulationMLP(nn.Module):
    def __init__(self, in_features, hidden_features, hidden_layers,
                 first_omega_0=30,
                 hidden_omega_0=30.):
        super().__init__()

        self.hidden_layers = hidden_layers
        self.first_layer = SineLayer(in_features, hidden_features,
                                     is_first=True, omega_0=first_omega_0)

        for i in range(hidden_layers):
            layer = SineLayer(hidden_features, hidden_features,
                              is_first=False, omega_0=hidden_omega_0)
            setattr(self, f'layer_{i}', layer)

    def forward(self, coords):
        output = self.first_layer(coords)
        skip = output
        gating_layers = []
        for i in range(self.hidden_layers):
            output = getattr(self, f'layer_{i}')(output) + skip
            gating_layers.append(output)
        return gating_layers


class ModSIREN(nn.Module):
    def __init__(self, in_features, out_features, hidden_layers=4, hidden_features=256, latent_dim=0,
                 synthesis_activation=None, modulation_activation=None, concat=True, pos_embed='identity', num_freqs=None, freq_scale=1.):
        super().__init__()

        if pos_embed == 'ffm':
            self.pos_embed = FourierFeatMapping(in_features, num_freqs, freq_scale)
        elif pos_embed == 'pe':
            self.pos_embed = PositionalEncoding(in_features, num_freqs)
        else:
            self.pos_embed = IdentityMapping(in_features)
        total_in_features = self.pos_embed.out_dim
        self.concat = concat

        if modulation_activation:
            if synthesis_activation == 'sine':
                first_omega_0 = 30. / freq_scale
                hidden_omega_0 = 30. / freq_scale
                self.synthesis_nw = GatedSineMLP(total_in_features, out_features, hidden_layers, hidden_features,
                                                 outermost_linear=True, first_omega_0=freq_scale, hidden_omega_0=hidden_omega_0)
            elif synthesis_activation == 'relu':
                self.synthesis_nw = GatedReluMLP(total_in_features, out_features, hidden_layers, hidden_features,
                                                 outermost_linear=True)
            else:
                self.synthesis_nw = ReLUMLP(total_in_features, out_features, hidden_layers, hidden_features)

            if modulation_activation == 'relu':
                if concat:
                    self.modulation_nw = ModulationMLP(latent_dim + total_in_features, hidden_features, hidden_layers)
                else:
                    self.modulation_nw = ModulationMLP(latent_dim, hidden_features, hidden_layers)
            else:
                if concat:
                    self.modulation_nw = SineModulationMLP(latent_dim + total_in_features, hidden_features, hidden_layers)
                else:
                    self.modulation_nw = SineModulationMLP(latent_dim, hidden_features, hidden_layers)
        else:
            self.modulation_nw = None
            if synthesis_activation == 'sine':
                self.synthesis_nw = SineMLP(total_in_features + latent_dim, out_features, hidden_layers, hidden_features, outermost_linear=True)
            else:
                self.synthesis_nw = ReLUMLP(total_in_features + latent_dim, out_features, hidden_layers, hidden_features)

    def forward(self, coords, embedding=None):
        coords = self.pos_embed(coords)
        # print(f'coords: {coords.shape}')

        latent_vec = embedding
        if latent_vec is not None:
            if len(latent_vec.shape) != len(coords.shape):
                BS, PS, D = coords.shape
                latent_vec = latent_vec.unsqueeze(-2).repeat(1, PS, 1)

            gating_layers = None
            if self.modulation_nw:
                if self.concat:
                    gating_layers = self.modulation_nw(torch.cat([latent_vec, coords], dim=-1))
                else:
                    gating_layers = self.modulation_nw(latent_vec)
                model_output = self.synthesis_nw(coords, gating_layers)
            else:
                model_output = self.synthesis_nw(torch.cat([latent_vec, coords], dim=-1), gating_layers)
        else:
            model_output = self.synthesis_nw(coords)

        return model_output
