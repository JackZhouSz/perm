import os

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
from pytorch_lightning.utilities import rank_zero_only

from hair import HairRoots, Strands, save_hair
from hair.rotational_repr import cartesian_to_rotational_repr
from models.loss import StrandReconstructionLoss
from models.module import Codebook, INRMoE, ModSIREN, PIFuDecoder, Transformer
from models.strand_codec import StrandCodec
from utils.blend_shape import sample, solve_blend_shapes
from utils.image import write_png
from utils.metric import export_csv, hair_reconstruction_metrics
from utils.misc import copy2cpu as c2c
from utils.misc import load_checkpoint
from utils.visualize import visualize_explained_variance


class NeuralTextureModSIREN(pl.LightningModule):
    def __init__(self, args):
        super().__init__()
        self.save_hyperparameters(args)

        if args.texture.conditioning == 'concatenation':
            self.texture = ModSIREN(in_features=2,
                                    out_features=args.texture.out_features,
                                    hidden_layers=args.texture.hidden_layers,
                                    hidden_features=args.texture.hidden_features,
                                    latent_dim=args.codebook.latent_dim,
                                    concat=False,
                                    synthesis_layer_norm=args.texture.synthesis_layer_norm,
                                    modulator_layer_norm=False,
                                    synthesis_nonlinearity=args.texture.synthesis_nonlinearity,
                                    modulator_nonlinearity=None,
                                    pos_embed=args.texture.pos_embed,
                                    num_freqs=args.texture.num_freqs,
                                    freq_scale=args.texture.freq_scale
                                    )
        elif args.texture.conditioning == 'modulation':
            self.texture = ModSIREN(in_features=2,
                                    out_features=args.texture.out_features,
                                    hidden_layers=args.texture.hidden_layers,
                                    hidden_features=args.texture.hidden_features,
                                    latent_dim=args.codebook.latent_dim,
                                    concat=args.texture.concat,
                                    synthesis_layer_norm=args.texture.synthesis_layer_norm,
                                    modulator_layer_norm=args.texture.modulator_layer_norm,
                                    synthesis_nonlinearity=args.texture.synthesis_nonlinearity,
                                    modulator_nonlinearity=args.texture.modulator_nonlinearity,
                                    pos_embed=args.texture.pos_embed,
                                    num_freqs=args.texture.num_freqs,
                                    freq_scale=args.texture.freq_scale
                                    )
        elif args.texture.conditioning == 'attention':
            self.texture = Transformer(in_features=2,
                                       out_features=args.texture.out_features,
                                       num_attention_layers=args.texture.num_attention_layers,
                                       token_dim=args.texture.token_dim,
                                       num_heads=args.texture.num_heads,
                                       dim_head=args.texture.dim_head,
                                       self_attention=args.texture.self_attention,
                                       offset_attention=args.texture.offset_attention,
                                       pre_norm=args.texture.pre_norm,
                                       admin_init=args.texture.admin_init,
                                       pos_embed=args.texture.pos_embed,
                                       num_freqs=args.texture.num_freqs,
                                       freq_scale=args.texture.freq_scale
                                       )
        else:
            self.texture = None

        if args.texture.pifu is True:
            self.pifu = PIFuDecoder(in_features=args.codebook.latent_dim,
                                    out_features=args.pifu.out_features,
                                    hidden_features=args.pifu.hidden_features,
                                    kernel_size=args.pifu.kernel_size,
                                    bilinear=args.pifu.bilinear,
                                    batch_norm=args.pifu.batch_norm,
                                    nonlinearity=args.pifu.nonlinearity
                                    )
        else:
            self.pifu = None

        if args.strand.strand_repr == 'feature':
            self.strand_codec = load_checkpoint(StrandCodec, args.dataset.codec_ckpt)
            self.strand_codec.eval()
            self.strand_codec.freeze()

        self.codebook = Codebook(args.codebook.num_entries, args.codebook.latent_dim)

        # self.criterion_strand = StrandReconstructionLoss(cos_angle=True, reduction='mean')
        self.criterion_recon = nn.L1Loss(reduction='mean')

        self.validation_step_outputs = dict()

    def forward(self, coords, embedding, decode_strands=False):
        B, N = coords.shape[:2]

        # either use a global embedding or a set of spatial emebddings
        if self.pifu is not None:
            pifu_features = self.pifu(embedding)  # (B, C, H, W)
            embedding = sample(coords, pifu_features, self.hparams.texture['interp_mode'])  # (B, N, C)

        # either sample implicit neural textures or use PIFu outputs directly
        if self.texture is not None:
            texture = self.texture(coords, embedding)
        else:
            texture = embedding

        output = {'feature': texture}
        if decode_strands:
            strand_repr = self.hparams.strand['strand_repr']
            if strand_repr == 'feature':
                strands = self.strand_codec.decode(texture.reshape(B * N, -1))
                output['strands'] = strands.reshape(B, N)
            else:
                texture = texture.reshape(B, N, self.hparams.strand['samples_per_strand'], -1)
                global_rot = self.hparams.strand.get('global_rot', False)
                output['strands'] = Strands.from_tensor(texture, strand_repr, global_rot)

        return output

    def on_train_start(self):
        self.hair_roots = HairRoots(head_mesh=self.hparams.dataset['head_mesh'], scalp_bounds=self.hparams.dataset['scalp_bounds'])

    def training_step(self, batch, batch_idx):
        coords = self.hair_roots.scale(batch['root'][..., :2])
        embedding = self.codebook(batch['idx'].reshape(-1).long())
        output = self(coords, embedding, decode_strands=False)

        gt_feature = sample(coords, batch['feature'], self.hparams.texture['interp_mode'])
        terms = {'feat': self.criterion_recon(output['feature'], gt_feature)}
        if self.hparams.optimizer['lambda_reg'] != 0:
            terms.update({'reg': torch.mean(embedding**2)})

        loss = 0
        for k, v in terms.items():
            loss += self.hparams.optimizer[f'lambda_{k}'] * v
            self.log(f'{k}', v, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)

        return loss

    def write_texture(self, embedding, filename, resolution):
        u, v = torch.meshgrid(torch.linspace(0, 1, steps=resolution, device=embedding.device),
                              torch.linspace(0, 1, steps=resolution, device=embedding.device), indexing='ij')
        coords = torch.dstack((u, v)).reshape(-1, 2)  # (W x H, 2)
        texture = self(coords.unsqueeze(0), embedding, decode_strands=False)['feature']  # (1, W x H, C)
        texture = texture.reshape(1, resolution, resolution, -1)  # (1, W, H, C)

        v_min, v_max = texture.min(), texture.max()
        texture = (texture - v_min) / (v_max - v_min)  # (1, W, H, C), scale to [0, 1]
        texture = texture.transpose(1, 2)  # (1, H, W, C)

        write_png(filename, texture[0, :, :, :3])

    @rank_zero_only
    def on_validation_start(self):
        self.val_dir = os.path.join(self.logger.log_dir, 'val')
        os.makedirs(self.val_dir, exist_ok=True)
        self.hair_roots = HairRoots(head_mesh=self.hparams.dataset['head_mesh'], scalp_bounds=self.hparams.dataset['scalp_bounds'])

    @rank_zero_only
    def validation_step(self, batch, batch_idx):
        coords = self.hair_roots.scale(batch['root'][..., :2])
        embedding = self.codebook(batch['idx'].reshape(-1).long())
        texture_type = self.hparams.texture['type']
        if texture_type == 'local':
            local_strands = self(coords, embedding, decode_strands=True)['strands']
            local_strands.residual = batch['residual_local']
            guide_strands = Strands(rotation=batch['rotation_guide'], length=batch['length_guide'])
            recon_strands = local_strands.to_world(guide_strands)
        elif texture_type == 'guide':
            local_strands = Strands(position=batch['position_local'], rotation=batch['rotation_local'], length=batch['length_local'], residual=batch['residual_local'])
            guide_strands = self(coords, embedding, decode_strands=True)['strands']
            recon_strands = local_strands.to_world(guide_strands)
        elif texture_type == 'residual':
            local_strands = Strands(position=batch['position_local'], rotation=batch['rotation_local'], length=batch['length_local'])
            residual_strands = self(coords, embedding, decode_strands=True)['strands']
            local_strands.residual = residual_strands.residual
            guide_strands = Strands(rotation=batch['rotation_guide'], length=batch['length_guide'])
            recon_strands = local_strands.to_world(guide_strands)
        else:
            recon_strands = self(coords, embedding, decode_strands=True)['strands']

        x0 = self.hair_roots.spherical_to_cartesian(batch['root'])  # (B, N, 3)
        position = recon_strands.position + x0.unsqueeze(2)
        position_gt = F.pad(batch['position'], (0, 0, 1, 0), mode='constant', value=0)
        position_gt = position_gt + x0.unsqueeze(2)
        metric = hair_reconstruction_metrics(c2c(position), c2c(position_gt))
        for k, v in metric.items():
            if k in self.validation_step_outputs:
                self.validation_step_outputs[k].append(v)
            else:
                self.validation_step_outputs[k] = [v]

        if batch_idx <= 15:
            batch_size = x0.shape[0]
            for i in range(batch_size):
                save_hair(os.path.join(self.val_dir, f'hair_{batch_idx * batch_size + i:05d}_gt.ply'), c2c(position_gt[i]))
                save_hair(os.path.join(self.val_dir, f'hair_{batch_idx * batch_size + i:05d}_recon.ply'), c2c(position[i]))
                self.write_texture(embedding[i:i + 1], os.path.join(self.val_dir, f'hair_{batch_idx * batch_size + i:05d}.png'), self.hparams.texture['texture_size'])

    @rank_zero_only
    def on_validation_epoch_end(self):
        export_csv(os.path.join(self.val_dir, 'metrics.csv'), self.validation_step_outputs)

        mean_shape = self.codebook.data.mean(dim=0, keepdims=True)
        blend_shapes, explained_variance_ratio = solve_blend_shapes(c2c(self.codebook.data - mean_shape), self.hparams.codebook['num_parameters'])
        visualize_explained_variance(os.path.join(self.logger.log_dir, 'variance.png'), explained_variance_ratio)
        np.savez(os.path.join(self.logger.log_dir, 'blend_shapes.npz'), mean_shape=c2c(mean_shape), blend_shapes=blend_shapes, variance_ratio=explained_variance_ratio)

    def configure_optimizers(self):
        model_parameters = list()
        if self.texture is not None:
            model_parameters += list(self.texture.parameters())
        if self.pifu is not None:
            model_parameters += list(self.pifu.parameters())
        optimizer = torch.optim.Adam(
            [
                {'params': model_parameters, 'lr': self.hparams.optimizer['model_lr']},
                {'params': self.codebook.parameters(), 'lr': self.hparams.optimizer['codebook_lr']}
            ]
        )

        if self.hparams.scheduler['name'] is None:
            return optimizer
        else:
            if self.hparams.scheduler['name'] == 'step':
                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, self.hparams.scheduler['step_size'], self.hparams.scheduler['gamma'], verbose=False)
            else:
                raise NotImplementedError(f"lr scheduler {self.hparams.scheduler['name']} is unsupported")

            return [optimizer], [scheduler]


class NeuralTextureMoE(pl.LightningModule):
    def __init__(self, args):
        super().__init__()
        self.save_hyperparameters(args)

        self.texture = INRMoE(in_features=2,
                              out_features=args.texture.out_features,
                              hidden_layers=args.texture.hidden_layers,
                              hidden_features=args.texture.hidden_features,
                              num_experts=args.codebook.num_experts,
                              layer_norm=args.texture.layer_norm,
                              nonlinearity=args.texture.nonlinearity,
                              pos_embed=args.texture.pos_embed,
                              num_freqs=args.texture.num_freqs,
                              freq_scale=args.texture.freq_scale
                              )

        if args.strand.strand_repr == 'feature':
            self.strand_codec = load_checkpoint(StrandCodec, args.dataset.codec_ckpt)
            self.strand_codec.eval()
            self.strand_codec.freeze()

        # self.codebook = Codebook(args.codebook.num_entries, args.codebook.num_experts + 1)
        self.codebook = Codebook(args.codebook.num_entries, args.codebook.num_experts)

        self.criterion_recon = nn.L1Loss(reduction='mean')

        self.validation_step_outputs = dict()

    def forward(self, coords, coeff, decode_strands=False):
        B, N = coords.shape[:2]

        texture = self.texture(coords, coeff)
        output = {'feature': texture}

        if decode_strands:
            strand_repr = self.hparams.strand['strand_repr']
            if strand_repr == 'feature':
                strands = self.strand_codec.decode(texture.reshape(B * N, -1))
                output['strands'] = strands.reshape(B, N)
            else:
                texture = texture.reshape(B, N, self.hparams.strand['samples_per_strand'], -1)
                global_rot = self.hparams.strand.get('global_rot', False)
                output['strands'] = Strands.from_tensor(texture, strand_repr, global_rot)

        return output

    def on_train_start(self):
        self.hair_roots = HairRoots(head_mesh=self.hparams.dataset['head_mesh'], scalp_bounds=self.hparams.dataset['scalp_bounds'])

    def training_step(self, batch, batch_idx):
        coords = self.hair_roots.scale(batch['root'][..., :2])
        coeff = self.codebook(batch['idx'].reshape(-1).long())
        output = self(coords, coeff, decode_strands=False)

        gt_feature = sample(coords, batch['feature'], self.hparams.texture['interp_mode'])
        loss = self.criterion_recon(output['feature'], gt_feature)
        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)

        return loss

    def write_texture(self, coeff, filename, resolution):
        u, v = torch.meshgrid(torch.linspace(0, 1, steps=resolution, device=coeff.device),
                              torch.linspace(0, 1, steps=resolution, device=coeff.device), indexing='ij')
        coords = torch.dstack((u, v)).reshape(-1, 2)  # (W x H, 2)
        texture = self(coords.unsqueeze(0), coeff, decode_strands=False)['feature']  # (1, W x H, C)
        texture = texture.reshape(1, resolution, resolution, -1)  # (1, W, H, C)

        v_min, v_max = texture.min(), texture.max()
        texture = (texture - v_min) / (v_max - v_min)  # (1, W, H, C), scale to [0, 1]
        texture = texture.transpose(1, 2)  # (1, H, W, C)

        write_png(filename, texture[0, :, :, :3])

    @rank_zero_only
    def on_validation_start(self):
        self.val_dir = os.path.join(self.logger.log_dir, 'val')
        os.makedirs(self.val_dir, exist_ok=True)
        self.hair_roots = HairRoots(head_mesh=self.hparams.dataset['head_mesh'], scalp_bounds=self.hparams.dataset['scalp_bounds'])

    @rank_zero_only
    def validation_step(self, batch, batch_idx):
        coords = self.hair_roots.scale(batch['root'][..., :2])
        coeff = self.codebook(batch['idx'].reshape(-1).long())
        texture_type = self.hparams.texture['type']
        if texture_type == 'local':
            local_strands = self(coords, coeff, decode_strands=True)['strands']
            local_strands.residual = batch['residual_local']
            guide_strands = Strands(rotation=batch['rotation_guide'], length=batch['length_guide'])
            recon_strands = local_strands.to_world(guide_strands)
        elif texture_type == 'guide':
            local_strands = Strands(position=batch['position_local'], rotation=batch['rotation_local'], length=batch['length_local'], residual=batch['residual_local'])
            guide_strands = self(coords, coeff, decode_strands=True)['strands']
            recon_strands = local_strands.to_world(guide_strands)
        elif texture_type == 'residual':
            local_strands = Strands(position=batch['position_local'], rotation=batch['rotation_local'], length=batch['length_local'])
            residual_strands = self(coords, coeff, decode_strands=True)['strands']
            local_strands.residual = residual_strands.residual
            guide_strands = Strands(rotation=batch['rotation_guide'], length=batch['length_guide'])
            recon_strands = local_strands.to_world(guide_strands)
        else:
            recon_strands = self(coords, coeff, decode_strands=True)['strands']

        x0 = self.hair_roots.spherical_to_cartesian(batch['root'])  # (B, N, 3)
        position = recon_strands.position + x0.unsqueeze(2)
        position_gt = F.pad(batch['position'], (0, 0, 1, 0), mode='constant', value=0)
        position_gt = position_gt + x0.unsqueeze(2)
        metric = hair_reconstruction_metrics(c2c(position), c2c(position_gt))
        for k, v in metric.items():
            if k in self.validation_step_outputs:
                self.validation_step_outputs[k].append(v)
            else:
                self.validation_step_outputs[k] = [v]

        if batch_idx < 100:
            batch_size = x0.shape[0]
            for i in range(batch_size):
                save_hair(os.path.join(self.val_dir, f'hair_{batch_idx * batch_size + i:05d}_gt.ply'), c2c(position_gt[i]))
                save_hair(os.path.join(self.val_dir, f'hair_{batch_idx * batch_size + i:05d}_recon.ply'), c2c(position[i]))
                self.write_texture(coeff[i:i + 1], os.path.join(self.val_dir, f'hair_{batch_idx * batch_size + i:05d}.png'), self.hparams.texture['texture_size'])

    @rank_zero_only
    def on_validation_epoch_end(self):
        export_csv(os.path.join(self.val_dir, 'metrics.csv'), self.validation_step_outputs)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            [
                {'params': self.texture.parameters(), 'lr': self.hparams.optimizer['texture_lr']},
                {'params': self.codebook.parameters(), 'lr': self.hparams.optimizer['codebook_lr']}
            ]
        )

        if self.hparams.scheduler['name'] is None:
            return optimizer
        else:
            if self.hparams.scheduler['name'] == 'step':
                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, self.hparams.scheduler['step_size'], self.hparams.scheduler['gamma'], verbose=False)
            else:
                raise NotImplementedError(f"lr scheduler {self.hparams.scheduler['name']} is unsupported")

            return [optimizer], [scheduler]
