import os
from typing import Optional

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
from pytorch_lightning.utilities import rank_zero_only

from hair import HairRoots, Strands, save_hair
from hair.loss import StrandLoss
from models.module import ModSIREN, TextureDecoder, TextureEncoder, Transformer
from models.strand_codec import StrandCodec
from utils.blend_shape import blend, sample
from utils.metric import export_csv, hair_reconstruction_metrics
from utils.misc import copy2cpu as c2c
from utils.misc import destandardize, standardize
from utils.patch import assemble_patches
from utils.visualize import write_texture


class NeuralTexturePCA(nn.Module):
    TEXTURE_SIZE = 256

    def __init__(self, model_path: Optional[str] = None, num_coeff: Optional[int] = None, texture_type: str = 'texture'):
        super().__init__()

        if model_path is not None:
            data = np.load(model_path)
            self.register_buffer('mean_shape', torch.tensor(data['mean_shape'], dtype=torch.float32))

            if num_coeff is not None:
                num_coeff = min(num_coeff, data['blend_shapes'].shape[0])
            else:
                num_coeff = data['blend_shapes'].shape[0]
            self.register_buffer('blend_shapes', torch.tensor(data['blend_shapes'][:num_coeff], dtype=torch.float32))
        else:
            print(f'WARNING: You are using a `NeuralTexture` model without blend shapes, thus you need to optimize each texel directly.')

        if texture_type == 'texture':
            assert self.TEXTURE_SIZE == data['blend_shapes'].shape[-1], \
                f"You are using a `NeuralTexture` model with texture size {data['blend_shapes'].shape[-1]}, while the default value is {self.TEXTURE_SIZE}."
            self._num_coeff = num_coeff
        elif texture_type == 'patch':
            patch_size = data['blend_shapes'].shape[-1]
            num_patches = int(2 * (self.TEXTURE_SIZE / patch_size) - 1) ** 2
            self._num_coeff = num_coeff * num_patches
        else:
            self._num_coeff = num_coeff * self.TEXTURE_SIZE * self.TEXTURE_SIZE
        self.texture_type = texture_type

    @property
    def num_coeff(self):
        return self._num_coeff

    def forward(self, coords, coeff):
        batch_size = coeff.shape[0]
        if self.texture_type == 'texture':
            blend_shape = self.mean_shape + blend(coeff, self.blend_shapes)  # (batch_size, feature_dim, height, width)
        elif self.texture_type == 'patch':
            coeff = coeff.reshape(-1, self.blend_shapes.shape[0])  # (batch_size x num_patches, num_blend_shapes)
            patches = self.mean_shape + blend(coeff, self.blend_shapes)  # (batch_size x num_patches, feature_dim, height, width)
            patches = patches.reshape(-1, batch_size, *patches.shape[1:])  # (num_patches, batch_size, feature_dim, height, width)
            blend_shape = assemble_patches(patches, overlap=True)  # (batch_size, feature_dim, height, width)
        else:
            blend_shape = coeff.reshape(batch_size, -1, self.TEXTURE_SIZE, self.TEXTURE_SIZE)  # (batch_size, feature_dim, height, width)
        # output = sample(coords, blend_shape, mode='bilinear')  # (batch_size, num_coords, feature_dim)
        output = sample(coords, blend_shape, mode='nearest')  # (batch_size, num_coords, feature_dim)

        return output


class NeuralTexture(pl.LightningModule):
    def __init__(self, args):
        super().__init__()
        self.save_hyperparameters(args)

        self.encoder = TextureEncoder(in_features=args.encoder.in_features,
                                      out_features=args.latent.dim,
                                      hidden_features=args.encoder.hidden_features,
                                      kernel_size=args.encoder.kernel_size,
                                      pooling=args.encoder.pooling,
                                      spatial=args.encoder.spatial,
                                      batch_norm=args.encoder.batch_norm,
                                      nonlinearity=args.encoder.nonlinearity
                                      )

        conditioning = args.texture.get('conditioning', None)
        if conditioning is None:
            self.texture = TextureDecoder(in_features=args.latent.dim,
                                          out_features=args.strand.num_coeff,
                                          hidden_features=args.texture.hidden_features,
                                          kernel_size=args.texture.kernel_size,
                                          spatial=args.encoder.spatial,
                                          bilinear=args.texture.bilinear,
                                          batch_norm=args.texture.batch_norm,
                                          nonlinearity=args.texture.nonlinearity
                                          )
        else:
            if conditioning == 'concatenation':
                self.texture = ModSIREN(in_features=2,
                                        out_features=args.strand.num_coeff,
                                        hidden_layers=args.texture.hidden_layers,
                                        hidden_features=args.texture.hidden_features,
                                        latent_dim=args.latent.dim,
                                        concat=False,
                                        synthesis_layer_norm=args.texture.synthesis_layer_norm,
                                        modulator_layer_norm=False,
                                        synthesis_nonlinearity=args.texture.synthesis_nonlinearity,
                                        modulator_nonlinearity=None,
                                        pos_embed=args.texture.pos_embed,
                                        num_freqs=args.texture.num_freqs,
                                        freq_scale=args.texture.freq_scale
                                        )
            elif conditioning == 'modulation':
                self.texture = ModSIREN(in_features=2,
                                        out_features=args.strand.num_coeff,
                                        hidden_layers=args.texture.hidden_layers,
                                        hidden_features=args.texture.hidden_features,
                                        latent_dim=args.latent.dim,
                                        concat=args.texture.concat,
                                        synthesis_layer_norm=args.texture.synthesis_layer_norm,
                                        modulator_layer_norm=args.texture.modulator_layer_norm,
                                        synthesis_nonlinearity=args.texture.synthesis_nonlinearity,
                                        modulator_nonlinearity=args.texture.modulator_nonlinearity,
                                        pos_embed=args.texture.pos_embed,
                                        num_freqs=args.texture.num_freqs,
                                        freq_scale=args.texture.freq_scale
                                        )
            elif conditioning == 'attention':
                self.texture = Transformer(in_features=2,
                                           out_features=args.strand.num_coeff,
                                           num_attention_layers=args.texture.num_attention_layers,
                                           token_dim=args.texture.token_dim,
                                           num_heads=args.texture.num_heads,
                                           dim_head=args.texture.dim_head,
                                           self_attention=args.texture.self_attention,
                                           offset_attention=args.texture.offset_attention,
                                           pre_norm=args.texture.pre_norm,
                                           admin_init=args.texture.admin_init,
                                           pos_embed=args.texture.pos_embed,
                                           num_freqs=args.texture.num_freqs,
                                           freq_scale=args.texture.freq_scale
                                           )
            else:
                raise NotImplementedError(f'Found an unsupported latent conditioning method: {conditioning}')

        u, v = torch.meshgrid(torch.linspace(0, 1, steps=args.texture.texture_size, dtype=torch.float32),
                              torch.linspace(0, 1, steps=args.texture.texture_size, dtype=torch.float32), indexing='ij')
        tex_coords = torch.dstack((u, v)).reshape(1, -1, 2)  # (1, W x H, 2)
        self.register_buffer('tex_coords', tex_coords)

        mean_std_file = args.dataset.get('mean_std_file', None)
        if mean_std_file is not None:
            mean_std = np.load(mean_std_file)
            self.register_buffer('mean', torch.tensor(mean_std['mean'], dtype=torch.float32))
            self.register_buffer('std', torch.tensor(mean_std['std'], dtype=torch.float32))
            self.standardize_data = True
        else:
            self.standardize_data = False

        self.strand_codec = StrandCodec(args.strand.model_path, args.strand.num_coeff, args.strand.fft)

        self.criterion_strand = StrandLoss(reduction='mean')
        self.criterion_tex = nn.L1Loss(reduction='mean')

        self.validation_step_outputs = dict()

    @property
    def num_coeff(self):
        return self.hparams.latent['dim']

    # def forward(self, coords, embedding):
    #     batch_size, num_coords = coords.shape[:2]
    #     conditioning = self.hparams.texture.get('conditioning', None)
    #     if conditioning is None:
    #         texture = self.texture(embedding)  # (N, C, H, W)
    #     else:
    #         tex_coords = self.tex_coords.expand(batch_size, -1, -1)
    #         if embedding.ndim == 4:  # a spatial texture is encoded as the embedding (N, C, H, W), sample it to obtain an embedding for each point
    #             embedding = sample(tex_coords, embedding, mode='bilinear')
    #         texture = self.texture(tex_coords, embedding)  # (N, W x H, C)
    #         texture = texture.reshape(batch_size, self.hparams.texture['texture_size'], self.hparams.texture['texture_size'], -1)  # (N, W, H, C)
    #         texture = texture.permute(0, 3, 2, 1)  # (N, C, H, W)
    #     if self.standardize_data:
    #         texture = destandardize(texture, self.mean, self.std)
    #     output = {'texture': texture}

    #     coeff = sample(coords, texture, mode=self.hparams.texture['interp_mode'])
    #     position = self.strand_codec.decode(coeff.reshape(batch_size * num_coords, -1))
    #     position = position.reshape(batch_size, num_coords, -1, 3)
    #     position = F.pad(position, (0, 0, 1, 0), mode='constant', value=0)
    #     output['strands'] = Strands(position=position)

    #     return output

    def forward(self, coords, embedding):
        batch_size, num_coords = coords.shape[:2]
        conditioning = self.hparams.texture.get('conditioning', None)
        if conditioning is None:
            texture = self.texture(embedding)
            coeff = sample(coords, texture, mode=self.hparams.texture['interp_mode'])
        else:
            if embedding.ndim == 4:  # a spatial texture is encoded as the embedding (N, C, H, W), sample it to obtain an embedding for each point
                embedding = sample(coords, embedding, mode='bilinear')
            coeff = self.texture(coords, embedding)
        output = {'texture': coeff}

        position = self.strand_codec.decode(coeff.reshape(batch_size * num_coords, -1))
        position = position.reshape(batch_size, num_coords, -1, 3)
        position = F.pad(position, (0, 0, 1, 0), mode='constant', value=0)
        output['strands'] = Strands(position=position)

        return output

    def encode(self, texture):
        if self.standardize_data:
            texture = standardize(texture, self.mean, self.std)
        mean, logvar = self.encoder(texture)
        if self.hparams.latent['regularization'] == 'none':
            embedding = mean + 0.0 * logvar
        else:
            embedding = self.reparameterize(mean, logvar)

        return embedding, mean, logvar

    def reparameterize(self, mean, logvar):
        eps = torch.randn_like(mean)
        std = torch.exp(0.5 * logvar)

        return mean + eps * std

    def kl_loss(self, mean, logvar):
        loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)

        return loss.mean()

    def shared_step(self, batch, batch_idx, train=True):
        coords = self.hair_roots.scale(batch['roots'][..., :2])
        embedding, mean, logvar = self.encode(batch['texture'])
        output = self(coords, embedding)

        terms = dict()
        texture_type = self.hparams.texture['texture_type']
        if texture_type == 'local_tex':
            position = batch['local_strands']
        elif texture_type == 'guide_tex':
            position = batch['guide_strands']
            position = position.index_select(dim=0, index=batch['labels'].int())
        else:
            position = batch['strands']
        position = position - position[:, :, 0:1].clone()
        gt_strands = Strands(position=position)
        terms.update(self.criterion_strand(output['strands'], gt_strands))
        # terms.update({'tex': self.criterion_tex(output['texture'], batch['texture'])})
        gt_texture = sample(coords, batch['texture'], mode=self.hparams.texture['interp_mode'])
        terms.update({'tex': self.criterion_tex(output['texture'], gt_texture)})

        if self.hparams.latent['regularization'] != 'none':
            terms.update({'kl': self.kl_loss(mean, logvar)})

        if train:
            return terms, None, None
        else:
            return terms, gt_strands, output['strands']

    def on_train_start(self):
        self.hair_roots = HairRoots(head_mesh=self.hparams.head['head_mesh'], scalp_bounds=self.hparams.head['scalp_bounds'])

    def training_step(self, batch, batch_idx):
        terms, _, _ = self.shared_step(batch, batch_idx, train=True)

        loss = 0
        for k, v in terms.items():
            loss += self.hparams.optimizer[f'lambda_{k}'] * v
            self.log(f'{k}', v, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)

        return loss

    def validation_step(self, batch, batch_idx):
        terms, gt_strands, strands = self.shared_step(batch, batch_idx, train=False)

        loss = 0
        for k, v in terms.items():
            loss += self.hparams.optimizer[f'lambda_{k}'] * v
        if 'val_loss' not in self.validation_step_outputs:
            self.validation_step_outputs['val_loss'] = []
        self.validation_step_outputs['val_loss'].append(loss.unsqueeze(0))

        pos_diff = torch.norm(strands.position - gt_strands.position, dim=-1).mean(dim=(1, 2))
        if 'pos_diff' not in self.validation_step_outputs:
            self.validation_step_outputs['pos_diff'] = []
        self.validation_step_outputs['pos_diff'].append(pos_diff)

    def on_validation_epoch_end(self):
        val_loss = torch.cat(self.validation_step_outputs['val_loss']).mean()
        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True, sync_dist=True)
        self.validation_step_outputs['val_loss'].clear()
        pos_diff = torch.cat(self.validation_step_outputs['pos_diff']).mean()
        self.log('pos_diff', pos_diff, on_epoch=True, prog_bar=True, sync_dist=True)
        self.validation_step_outputs['pos_diff'].clear()

    def write_texture(self, gt_texture, prefix):
        embedding, _, _ = self.encode(gt_texture)
        texture = self(self.tex_coords, embedding)['texture']  # (1, W x H, C)
        texture = texture.reshape(1, self.hparams.texture['texture_size'], self.hparams.texture['texture_size'], -1)
        texture = texture.permute(0, 3, 2, 1)  # (1, C, H, W)

        write_texture(f'{prefix}_recon.png', texture[0].permute(1, 2, 0))
        write_texture(f'{prefix}_gt.png', gt_texture[0].permute(1, 2, 0))

    def on_test_start(self):
        self.test_dir = os.path.join(self.logger.log_dir, 'test')
        os.makedirs(self.test_dir, exist_ok=True)
        self.test_step_outputs = dict()
        self.hair_roots = HairRoots(head_mesh=self.hparams.head['head_mesh'], scalp_bounds=self.hparams.head['scalp_bounds'])

    @rank_zero_only
    def test_step(self, batch, batch_idx):
        _, gt_strands, strands = self.shared_step(batch, batch_idx, train=False)
        texture_type = self.hparams.texture['texture_type']
        if texture_type == 'local_tex':
            guide_strands = Strands(position=batch['guide_strands'])
            guide_strands = guide_strands.index_select(dim=0, index=batch['labels'].int())
            strands = strands.to_world(guide_strands)
        elif texture_type == 'guide_tex':
            local_strands = Strands(position=batch['local_strands'])
            strands = local_strands.to_world(strands)

        metric = hair_reconstruction_metrics(c2c(strands.position), c2c(gt_strands.position))
        for k, v in metric.items():
            if k in self.test_step_outputs:
                self.test_step_outputs[k].append(v)
            else:
                self.test_step_outputs[k] = [v]
        roots = self.hair_roots.spherical_to_cartesian(batch['roots'])  # (B, N, 3)
        strands.position = strands.position + roots.unsqueeze(2)
        gt_strands.position = gt_strands.position + roots.unsqueeze(2)

        if batch_idx <= 15:
            batch_size = roots.shape[0]
            for i in range(batch_size):
                save_hair(os.path.join(self.test_dir, f'hair_{batch_idx * batch_size + i:05d}_gt.ply'), c2c(gt_strands.position[i]))
                save_hair(os.path.join(self.test_dir, f'hair_{batch_idx * batch_size + i:05d}_recon.ply'), c2c(strands.position[i]))
                self.write_texture(batch['texture'], os.path.join(self.test_dir, f'hair_{batch_idx * batch_size + i:05d}'))

    @rank_zero_only
    def on_test_epoch_end(self):
        export_csv(os.path.join(self.test_dir, 'metrics.csv'), self.test_step_outputs)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.optimizer['lr'], weight_decay=self.hparams.optimizer['weight_decay'])

        scheduler_args = self.hparams.get('scheduler', None)
        if scheduler_args is None:
            return optimizer
        else:
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, scheduler_args['step_size'], scheduler_args['gamma'], verbose=False)
            return [optimizer], [scheduler]
